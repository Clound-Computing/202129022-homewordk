import json
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# 下载NLTK的停用词和Punkt Tokenizer
nltk.download('stopwords')
nltk.download('punkt')

#加载数据集
# 读取JSON文件
with open('dataset.json', 'r',encoding='utf-8') as f:
    data = json.load(f)

# 处理数据，提取'text'和'description'
processed_data = []
for key, item in data.items():
    if isinstance(item, dict) and 'text' in item and 'label' in item:
        processed_data.append({
            'text': item['text'],
            'label': item['label']
        })

# 只保留前300条数据
processed_data = processed_data[2784:4784]

# 清洗文本：删除多余的换行符和特殊字符
def clean_text(text):
    text = re.sub(r'\n+', ' ', text)  # 删除多余的新行
    text = re.sub(r'\s+', ' ', text)  # 替换多个空格为单个空格
    return text.strip()

# 文本长度限制
max_length = 25

# 定义数据清洗和预处理的函数
def preprocess_text(text):
    # 清洗文本：删除多余的换行符和特殊字符
    text = re.sub(r'\n+', ' ', text)  
    text = re.sub(r'\s+', ' ', text)  
    text = text.strip()  
    # 文本标准化：转换为小写，移除标点，分词
    text = text.lower()  
    text = re.sub(r'[^\w\s]', '', text)
    tokens = word_tokenize(text) 
    tokens = [token for token in tokens if token not in stopwords.words('english')] 
    # 限制文本长度
    text = ' '.join(tokens[:max_length]) 
    return text

# 对数据集中的每个字典进行预处理
for item in processed_data:
    item['text'] = preprocess_text(item['text'])

# 遍历列表中的每个字典，并更新 'label' 键的值
for item in processed_data:
    if item['label'] == 'fake':
        item['label'] = 0
    elif item['label'] == 'real':
        item['label'] = 1

# 数据拆分
from datasets import DatasetDict
from datasets import Dataset
dataset = DatasetDict()
dataset = Dataset.from_list(processed_data)
dataset = dataset.train_test_split(train_size = 0.5, test_size=0.5, seed=42)

#加载数据
from openicl import DatasetReader
data=DatasetReader(dataset, input_columns='text', output_column='label')
